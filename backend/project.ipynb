{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rFbIuF6VOi3F",
   "metadata": {
    "id": "rFbIuF6VOi3F"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fa2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.43)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sympy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy\n",
    "%pip install sympy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "ec43006f-30f7-4395-e1e5-72eb36faa965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.3.75)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.dialects.mysql.mariadb import loader\n",
    "from sympy.codegen.fnodes import dimension\n",
    "%pip install langchain_community\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3641dbd6e23b6f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3641dbd6e23b6f4",
    "outputId": "a06b2b7c-8132-4693-966b-4f52a186ac3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VsCodeProjects\\Quran-Hadith-Bot\\backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60ae01780d47f",
   "metadata": {
    "id": "ef60ae01780d47f"
   },
   "outputs": [],
   "source": [
    "##commented code is not unnecessary , it was onetime executed code ,for formatting and prerpocessing of data\n",
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# loader=CSVLoader(file_path=\"/content/all_hadiths_clean.csv\",encoding=\"utf-8\")\n",
    "# docs=loader.load()\n",
    "# print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a3a0a8f729d188",
   "metadata": {
    "id": "53a3a0a8f729d188"
   },
   "outputs": [],
   "source": [
    "# print(type(docs))\n",
    "# print(len(docs[444].page_content.split()))\n",
    "# print(docs[444])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8140787ad838d84c",
   "metadata": {
    "id": "8140787ad838d84c"
   },
   "outputs": [],
   "source": [
    "# def countTokens(text):\n",
    "#     return len(text.split())\n",
    "# maxTokens = 0\n",
    "# maxIndex = -1\n",
    "# for i, doc in enumerate(docs):\n",
    "#     token_count = countTokens(doc.page_content)\n",
    "#     if token_count > maxTokens:\n",
    "#         maxTokens = token_count\n",
    "#         maxIndex = i\n",
    "\n",
    "# print(maxTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3052dc4ee158c21",
   "metadata": {
    "id": "f3052dc4ee158c21"
   },
   "outputs": [],
   "source": [
    "# print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b923ee51e80bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74b923ee51e80bd2",
    "outputId": "fbc3ce20-093b-4a18-e0f1-aedf03c31929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-huggingface sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f069845fc2668e4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f069845fc2668e4d",
    "outputId": "d03c9a99-ded2-4844-f9f0-9a87a850ba67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33d2db5e592a05b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f33d2db5e592a05b",
    "outputId": "dab22dd9-53a2-44ca-91d1-c6165e082fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330cfbb",
   "metadata": {},
   "source": [
    "QURAN DATA processing\n",
    "Loading\n",
    "creating document a=object manually {adding metadata and content}\n",
    "chunks creation for bigger ayahs that might gets truncated if not within token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80ed223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain.docstore.document import Document\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import math\n",
    "\n",
    "# def clean_value(value):\n",
    "#     if value is None:\n",
    "#         return 0\n",
    "#     if isinstance(value, float) and pd.isna(value):  # catches NaN\n",
    "#         return 0\n",
    "#     if str(value).lower() in [\"nan\", \"null\", \"none\"]:\n",
    "#         return 0\n",
    "#     return value\n",
    "\n",
    "# # 1. Load the CSV file into a Pandas DataFrame\n",
    "# df = pd.read_csv('The Quran Dataset.csv')\n",
    "\n",
    "# # 2. Create a list of LangChain Document objects manually\n",
    "# docs = []\n",
    "# for index, row in df.iterrows():\n",
    "\n",
    "#     # Construct the metadata dictionary\n",
    "#     metadata = {\n",
    "#     'surah_no': clean_value(row.get('surah_no')),\n",
    "#     'surah_name_en': clean_value(row.get('surah_name_en')),\n",
    "#     'surah_name_ar': clean_value(row.get('surah_name_ar')),\n",
    "#     'surah_name_roman': clean_value(row.get('surah_name_roman')),\n",
    "#     'ayah_no_surah': clean_value(row.get('ayah_no_surah')),\n",
    "#     'ayah_no_quran': clean_value(row.get('ayah_no_quran')),\n",
    "#     'ayah_ar': clean_value(row.get('ayah_ar')),\n",
    "#     'ayah_en': clean_value(row.get('ayah_en')),\n",
    "#     'ruko_no': clean_value(row.get('ruko_no')),\n",
    "#     'juz_no': clean_value(row.get('juz_no')),\n",
    "#     'manzil_no': clean_value(row.get('manzil_no')),\n",
    "#     'hizb_quarter': clean_value(row.get('hizb_quarter')),\n",
    "#     'total_ayah_surah': clean_value(row.get('total_ayah_surah')),\n",
    "#     'total_ayah_quran': clean_value(row.get('total_ayah_quran')),\n",
    "#     'place_of_revelation': clean_value(row.get('place_of_revelation')),\n",
    "#     'sajdah_ayah': clean_value(row.get('sajah_ayah')),\n",
    "#     'sajdah_no': clean_value(row.get('sajdah_no')),\n",
    "#     'no_of_word_ayah': clean_value(row.get('no_of_word_ayah')),\n",
    "#     'list_of_words': clean_value(row.get('list_of_words'))\n",
    "# }\n",
    "\n",
    "#      # Construct the page_content\n",
    "#     # This will be the text we want the embeddings to represent.\n",
    "#     page_content = f\"AyahArabic: {row['ayah_ar']}\\nAyahEnglish: {row['ayah_en']}\"\n",
    "\n",
    "#     # Create the Document object and add it to our list\n",
    "#     docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "# # 3. Now, pass this correctly structured list of documents to the splitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "\n",
    "# # This will split your documents correctly and preserve the metadata\n",
    "# all_Quran_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# # You can now check the size and content of your new chunks\n",
    "# print(f\"Number of chunks: {len(all_Quran_chunks)}\")\n",
    "# print(\"Example chunk:\",all_Quran_chunks[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c43972",
   "metadata": {},
   "source": [
    "Hadith data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7257b8b03c7073",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be7257b8b03c7073",
    "outputId": "1cbf4bb7-82a1-4541-f6e5-718d1ef24e63"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain.docstore.document import Document\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # 1. Load the CSV file into a Pandas DataFrame\n",
    "# df = pd.read_csv('all_hadiths_clean.csv')\n",
    "\n",
    "# # 2. Create a list of LangChain Document objects manually\n",
    "# docs = []\n",
    "# for index, row in df.iterrows():\n",
    "#     # Construct the metadata dictionary\n",
    "#     metadata = {\n",
    "#         'id': row['id'],\n",
    "#         'hadith_id': row['hadith_id'],\n",
    "#         'source': row['source'],\n",
    "#         'chapter_no': row['chapter_no'],\n",
    "#         'hadith_no': row['hadith_no'],\n",
    "#         'chapter': row['chapter'],\n",
    "#     }\n",
    "\n",
    "#     # Construct the page_content\n",
    "#     # This will be the text we want the embeddings to represent.\n",
    "#     page_content = f\"Arabic: {row['text_ar']}\\nEnglish: {row['text_en']}\"\n",
    "\n",
    "#     # Create the Document object and add it to our list\n",
    "#     docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "# # 3. Now, pass this correctly structured list of documents to the splitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "\n",
    "# # This will split your documents correctly and preserve the metadata\n",
    "# all_hadith_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# # You can now check the size and content of your new chunks\n",
    "# print(f\"Number of chunks: {len(all_hadith_chunks)}\")\n",
    "# print(\"Example chunk:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df411846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1eda4d6118d8d1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1eda4d6118d8d1d",
    "outputId": "9d14bb01-cf36-4d2a-fc1b-66e9329a9775"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-pinecone langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecea927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dotenv) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ecaabed348d92e9",
   "metadata": {
    "id": "3ecaabed348d92e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "load_dotenv()\n",
    "# This line checks if the Pinecone API key exists as a Colab Secret.\n",
    "# If you don't have a secret named PINECONE_API_KEY, it will raise an error.\n",
    "try:\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\"'PINECONE_API_KEY' not found. Please add your key to the Secrets panel.\") from e\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L8c4s6rPgvGR",
   "metadata": {
    "id": "L8c4s6rPgvGR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "_Rsg-qiuaHnp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Rsg-qiuaHnp",
    "outputId": "b54725ef-edec-4e04-a61e-876ca32c6f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "%pip install --upgrade --quiet  langchain langchain-huggingface sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9a0c6",
   "metadata": {},
   "source": [
    "Hadith index initialization in pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb075d7adcfd30b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeb075d7adcfd30b",
    "outputId": "3ad79b08-ed07-482c-965d-2552ae72e6de"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from pinecone import  ServerlessSpec\n",
    "\n",
    "# # 1. Initialize the Pinecone client\n",
    "# # This uses your API key and environment to connect.\n",
    "\n",
    "\n",
    "# # 2. Define your index parameters\n",
    "# index_name = \"hadith-rag-index\" # Choose a unique, descriptive name\n",
    "# dimension = 1024 # Match the output dimension of your embedding model\n",
    "# metric = \"cosine\" # Cosine similarity is a good default for semantic search\n",
    "\n",
    "# # 3. Check if the index already exists\n",
    "# if index_name not in pc.list_indexes().names():\n",
    "#     print(f\"Creating a new Pinecone index: {index_name}...\")\n",
    "#     pc.create_index(\n",
    "#         name=index_name,\n",
    "#         dimension=dimension,\n",
    "#         metric=metric,\n",
    "#         spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "#     )\n",
    "#     print(\"Index created successfully!\")\n",
    "# else:\n",
    "#     print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# # 4. Now you have an active index that you can connect to.\n",
    "# pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0486a",
   "metadata": {},
   "source": [
    "Hadith embeddings->pinecone index\n",
    "Adding embeddings of hadith chunks to the pinecone index (hadith rag index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3622f9c9db029d",
   "metadata": {
    "id": "4e3622f9c9db029d"
   },
   "outputs": [],
   "source": [
    "###CODE TO create and store embeddings in the vector store\n",
    "###commennted so that i dont accidentally recreate embeddings\n",
    "#=========######################################=============\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# embeddingModel= HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "# vectorStore=PineconeVectorStore(index=pinecone_index,embedding=embeddingModel)\n",
    "# vectorStore.add_documents(documents=all_hadith_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aZXvs0GhgGKH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "8c0d7af05629494184cd285ffefe603a",
      "6d662d77a3344bad861d77940c05ca91",
      "9faf1c71a3144498902335ddc0bfaf7f",
      "def569e74c324af18e39ccb3dec87eca",
      "a6cdccecbe8541c5bb61df45e7deb249",
      "453cfb49d348484fbac5b6899e4db18e",
      "c1d1f42a58ac4c11ad0428b209c8afe4",
      "d79546d2b55643119f6cf7c7cc7a3f75",
      "4d3e48a8144e4c44ad1f8c497abec515",
      "3c3ff3ba27f54287b141ca738378c15c",
      "8a52297acd2e41e8a60b1943431ccff4",
      "24811ab8e13d447382b17c684f89c13c",
      "0f47544d43af4a1da1b99c030e89fac8",
      "f123ea88219349b4918f11d1e9ac76f5",
      "3fcbd1d97470428c9a5012ca2dd905bc",
      "39e380bc1912464593e845b94b11a569",
      "89b366ad8f274b04ac06e96565e0b793",
      "e3348fd59c4d4a259aee8a52f80d011a",
      "26356ebb834845bca8ee5e5b390073d1",
      "2a66a99abb0b4fed80e16d44a073f675",
      "57753bd0e3bd48828a7fd8c9f4f3dd23",
      "df99e250dc3a48f2a5b2c4ee2ac01b57",
      "a07b28b7fb164a8dab26d38f8baf1e71",
      "97d123d38f3a4dc99da406a22180a685",
      "8ea45e93769a4c1abf9ddc0cce9ed383",
      "854c049d2d404bab808ffb18cca94ade",
      "fb8b3bc6e8514373965b9d0367d8537c",
      "9940f9b76b044a78a056a373400ae860",
      "5781962d82774896bbb5500d7a8f64ab",
      "968724efeef94e08bf45d279d8392f46",
      "c0e194d0e4144d5a8be281d0416fe957",
      "2153c0ae3f434eb9bc227d9041be9543",
      "c474ada6661e4528b3b6474098915ec5",
      "c3ecefa4bf394f849b3d328e5f4c678f",
      "1e0513ab12234b949c325461dc34207d",
      "a352e23907c44269aec826565da1c519",
      "6dbcbe04cca3466d9695ae1fc45d9406",
      "2c81fcecb7984978bbdfed7fec904bf4",
      "6ba72542c275492faa9f19de2ab2a848",
      "f545836ff0554bd695095f4b22150244",
      "642c63730b524ca08a78bf82c7065d3e",
      "9a7d2de15590457f8000846975d9649a",
      "22ee4c33d1a34b579d358e0397fd567f",
      "18bd2087dc0d4a18963b31d3615e691d",
      "1d8675ac50154ab79f192ee5ab2a04f5",
      "f836da227e8b4687bb4385769896a487",
      "5d17d53017194403b283e45743ad3ed2",
      "fad8c5fa6591414d96921a4a41d5741d",
      "2df13e35e6714b7484378d99355d82e9",
      "4a214e5678a74f2eaa7487cff2565465",
      "161b353470e24f6588b0051d69de164b",
      "9eff041798bf4a1c9de9cf2279b987e1",
      "496a43a8a6e1401b93db982cd9098a21",
      "b0c7825ed85d4d639b24ad06a015d5a0",
      "ee7c2200dc114df4911b3fa55d22abc7",
      "a863c85831954344af974ea647d6f493",
      "df28c0f798794318aeae28311fb83398",
      "33205572feb1439fa8e4f79ad3d2abf2",
      "e040e31bb6e74eeba4104c5efa07559b",
      "6ea42202996c4c5599377b00e6e090ee",
      "1aa223c761d84c17a77b6bd74a8b1431",
      "1f87bf40691a468b9c33be625d536f31",
      "319fc4f0649646518412a8277efb89b6",
      "bb8ed239ace544d091b37e0a3256731e",
      "a4ac5121122645c2a2955ea9a8ebcc67",
      "efd6080de65d493fa676ceba81837d86",
      "df9b7567c64e4dcab647d210ac334ac8",
      "dbd5765b79614596a3b59ccc9b344dc6",
      "a5fb24fe1ead4582bb0d6d18f11a1698",
      "fa045597ba1d4645968c91bf3db2ca48",
      "89d1c663341148079fbbf0f3f7560f6b",
      "030d20236d23424ba24dcb40c495c3d3",
      "59c0a752b3c346cfa35b959185f67074",
      "4ebc02bb853745178f2370530ab016e3",
      "946c137807c84a4dbb2d9691a4201941",
      "c36d942fcdd04eb2a74fdd34167a19c7",
      "c881a19444584f27894825c3cabd9604",
      "cbbd04ef1c6a4d53a6b41062a3447cce",
      "6ffe582d5b2442f3bd4b23b091e307e7",
      "40d588ba28514539b20af50cd8b5d356",
      "fc4c4129adcb4d58905a1773706cb541",
      "9d87723639bc424d8aa9deb14a564e23",
      "0949272d4b614b4fa31fa3691b18362e",
      "a9b44cc8d32848f5a2f23f7be920ec2c",
      "41863aa5f656403ea495d8d824ecbe9f",
      "9cbb7e3a1ce8491e9080aec64cf6c0b5",
      "dff0f1be1ab448b28d23204723d675e3",
      "fff088949f574764b0675a72cf78d636",
      "72a792b4b00d45378ea5c7f0781f6e48",
      "efd65d39e20d4227b1d4b4a87c7d10a5",
      "78d3d5bf5d1645729e7ab4a785da2f00",
      "3ba3e183774441609b15101a01580d52",
      "53ff9cbffb7041b6a32e36a00faaf835",
      "ba6cc230b4e8465096b2cd82d076be77",
      "2226c34d90be46dabad103d7fb9720d1",
      "ef0fb4d5032a46999dfcc0dd92d99033",
      "bf52d3891f8046bf9ad80cd0e2730813",
      "cabde116dac44520ba15e2af609466cf",
      "b1e96de94d764916bda32ffa70175d2b",
      "98e50991ca2f4e5bb4c493eb6ef00794",
      "33bc8ae3775447ff814a1d3451ed294e",
      "00e03ec9e8194d2d8f5172af1ee6a6bb",
      "7e665393d33b4bbb8fbb2b2d4edf334e",
      "16e33ab656f244a491ef61591fb85dd0",
      "0eac1507eb7d4585becc31e913d0714d",
      "c578dc85a56949da9b4f41f49512df67",
      "b6bbbda5287349ceabbea84ae159e8f0",
      "a40566a3574641c8b19127875ce68d59",
      "72a5c29c21644cf5b4091956016c405e",
      "b46b87f7c0b7451c9009cf14ee6fd9a6",
      "7ff78a5d7aef402bad75318b947903c5",
      "48befa508dcf4ed4a8c21acfe9076e88",
      "fc9c7b4b351f4ffebdf7dec89267cf47",
      "9c4225760d4c4e108f0971d06250924c",
      "1e30dc0428204a0299483966039266c4",
      "04678e98a46f423dbf0c5c29c0355d57",
      "d5d7eed432df4cbf9d142effa936850a",
      "75b49d4236d44b5f85056d1173459503",
      "fca871a717cb46bdbb38c8e8ce918b20",
      "782b12b830114b50bddf92409f51fad0",
      "2cac70296f4b4d2da4a6183359564838"
     ]
    },
    "id": "aZXvs0GhgGKH",
    "outputId": "68ddf39b-3eaa-4f03-94f7-4b407cbb7576"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "# Get API key from Colab secrets\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = \"hadith-rag-index\" # You should set your index name here\n",
    "\n",
    "# Set the Pinecone API key as an environment variable\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "\n",
    "# 1. Initialize your embedding model\n",
    "embeddingModel = HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "# 2. Re-initialize the vector store by connecting to the existing index\n",
    "# Note: The `from_existing_index` method does NOT take an API key as a parameter,\n",
    "# but relies on the PINECONE_API_KEY environment variable.\n",
    "vectorStore = PineconeVectorStore.from_existing_index(\n",
    "\n",
    "    index_name=pinecone_index_name,\n",
    "    embedding=embeddingModel\n",
    ")\n",
    "\n",
    "# # You can now perform a similarity search with the vectorStore object\n",
    "# query = \"how to pray namaz according to hadith\"\n",
    "# results = vectorStore.similarity_search(query, k=5)\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216a5ff",
   "metadata": {},
   "source": [
    "Quran index initialization in pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "VlzUJlQJ9fe_",
   "metadata": {
    "id": "VlzUJlQJ9fe_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'quran-rag-index' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pinecone import  ServerlessSpec\n",
    "\n",
    "# 1. Initialize the Pinecone client\n",
    "# This uses your API key and environment to connect.\n",
    "\n",
    "\n",
    "# 2. Define your index parameters\n",
    "Quran_index_name = \"quran-rag-index\" # Choose a unique, descriptive name\n",
    "dimension = 1024 # Match the output dimension of your embedding model\n",
    "metric = \"cosine\" # Cosine similarity is a good default for semantic search\n",
    "\n",
    "# 3. Check if the index already exists\n",
    "if Quran_index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating a new Pinecone index: {Quran_index_name}...\")\n",
    "    pc.create_index(\n",
    "        name=Quran_index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    print(\"Index created successfully!\")\n",
    "else:\n",
    "    print(f\"Index '{Quran_index_name}' already exists.\")\n",
    "\n",
    "# 4. Now you have an active index that you can connect to.\n",
    "pinecone_Quran_index = pc.Index(Quran_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404ed0d",
   "metadata": {},
   "source": [
    "below code is run only once in his lifetime , as it will simply add embeddings to the index \n",
    "running it every time will keep adding indexes over indexes i.e repeating quran or hadith embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f28a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###CODE TO create and store embeddings in the vector store\n",
    "# ###commented so that i dont accidentally recreate embeddings\n",
    "# #=========######################################=============\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# embeddingModel= HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "# Quran_vectorStore=PineconeVectorStore(index=pinecone_Quran_index,embedding=embeddingModel)\n",
    "# Quran_vectorStore.add_documents(documents=all_Quran_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadf1c5",
   "metadata": {},
   "source": [
    "below code is to initialize index after that we have created the index so that we dont have to run the above code everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b57349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "# Get API key from Colab secrets\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = \"quran-rag-index\" # You should set your index name here\n",
    "\n",
    "# Set the Pinecone API key as an environment variable\n",
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "\n",
    "# 1. Initialize your embedding model\n",
    "embeddingModel = HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "# 2. Re-initialize the vector store by connecting to the existing index\n",
    "# Note: The `from_existing_index` method does NOT take an API key as a parameter,\n",
    "# but relies on the PINECONE_API_KEY environment variable.\n",
    "Quran_vectorStore = PineconeVectorStore.from_existing_index(\n",
    "\n",
    "    index_name=pinecone_index_name,\n",
    "    embedding=embeddingModel\n",
    ")\n",
    "\n",
    "# # You can now perform a similarity search with the vectorStore object\n",
    "# query = \"how to pray namaz according to hadith\"\n",
    "# results = vectorStore.similarity_search(query, k=5)\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9493384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6feec282-09db-4615-83c9-dd55427a3ddd', metadata={'ayah_ar': '۞ إِنَّ ٱللَّهَ يَأْمُرُ بِٱلْعَدْلِ وَٱلْإِحْسَٰنِ وَإِيتَآئِ ذِى ٱلْقُرْبَىٰ وَيَنْهَىٰ عَنِ ٱلْفَحْشَآءِ وَٱلْمُنكَرِ وَٱلْبَغْىِ ۚ يَعِظُكُمْ لَعَلَّكُمْ تَذَكَّرُونَ', 'ayah_en': 'Indeed, Allah commands justice, grace, as well as courtesy to close relatives. He forbids indecency, wickedness, and aggression. He instructs you so perhaps you will be mindful.', 'ayah_no_quran': 1991.0, 'ayah_no_surah': 90.0, 'hizb_quarter': 111.0, 'juz_no': 14.0, 'list_of_words': '[۞,إِنَّ,ٱللَّهَ,يَأْمُرُ,بِٱلْعَدْلِ,وَٱلْإِحْسَٰنِ,وَإِيتَآئِ,ذِى,ٱلْقُرْبَىٰ,وَيَنْهَىٰ,عَنِ,ٱلْفَحْشَآءِ,وَٱلْمُنكَرِ,وَٱلْبَغْىِ,ۚ,يَعِظُكُمْ,لَعَلَّكُمْ,تَذَكَّرُونَ]', 'manzil_no': 3.0, 'no_of_word_ayah': 18.0, 'place_of_revelation': 'Meccan', 'ruko_no': 236.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'النحل', 'surah_name_en': 'The Bee', 'surah_name_roman': 'An-Nahl', 'surah_no': 16.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 128.0}, page_content='AyahArabic: ۞ إِنَّ ٱللَّهَ يَأْمُرُ بِٱلْعَدْلِ وَٱلْإِحْسَٰنِ وَإِيتَآئِ ذِى ٱلْقُرْبَىٰ وَيَنْهَىٰ عَنِ ٱلْفَحْشَآءِ وَٱلْمُنكَرِ وَٱلْبَغْىِ ۚ يَعِظُكُمْ لَعَلَّكُمْ تَذَكَّرُونَ\\nAyahEnglish: Indeed, Allah commands justice, grace, as well as courtesy to close relatives. He forbids indecency, wickedness, and aggression. He instructs you so perhaps you will be mindful.'),\n",
       " Document(id='faba2f47-f6c5-4aa4-8319-a09fd36ccc57', metadata={'ayah_ar': 'يَٰٓأَيُّهَا ٱلَّذِينَ ءَامَنُوا۟ كُونُوا۟ قَوَّٰمِينَ لِلَّهِ شُهَدَآءَ بِٱلْقِسْطِ ۖ وَلَا يَجْرِمَنَّكُمْ شَنَـَٔانُ قَوْمٍ عَلَىٰٓ أَلَّا تَعْدِلُوا۟ ۚ ٱعْدِلُوا۟ هُوَ أَقْرَبُ لِلتَّقْوَىٰ ۖ وَٱتَّقُوا۟ ٱللَّهَ ۚ إِنَّ ٱللَّهَ خَبِيرٌۢ بِمَا تَعْمَلُونَ', 'ayah_en': 'O believers! Stand firm for Allah and bear true testimony. Do not let the hatred of a people lead you to injustice. Be just! That is closer to righteousness. And be mindful of Allah. Surely Allah is All-Aware of what you do.', 'ayah_no_quran': 677.0, 'ayah_no_surah': 8.0, 'hizb_quarter': 43.0, 'juz_no': 6.0, 'list_of_words': '[يَٰٓأَيُّهَا,ٱلَّذِينَ,ءَامَنُوا۟,كُونُوا۟,قَوَّٰمِينَ,لِلَّهِ,شُهَدَآءَ,بِٱلْقِسْطِ,ۖ,وَلَا,يَجْرِمَنَّكُمْ,شَنَـَٔانُ,قَوْمٍ,عَلَىٰٓ,أَلَّا,تَعْدِلُوا۟,ۚ,ٱعْدِلُوا۟,هُوَ,أَقْرَبُ,لِلتَّقْوَىٰ,ۖ,وَٱتَّقُوا۟,ٱللَّهَ,ۚ,إِنَّ,ٱللَّهَ,خَبِيرٌۢ,بِمَا,تَعْمَلُونَ]', 'manzil_no': 2.0, 'no_of_word_ayah': 30.0, 'place_of_revelation': 'Medinan', 'ruko_no': 87.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'المائدة', 'surah_name_en': 'The Table Spread', 'surah_name_roman': \"Al-Ma'idah\", 'surah_no': 5.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 120.0}, page_content='AyahEnglish: O believers! Stand firm for Allah and bear true testimony. Do not let the hatred of a people lead you to injustice. Be just! That is closer to righteousness. And be mindful of Allah. Surely Allah is All-Aware of what you do.'),\n",
       " Document(id='7a05f744-df71-4bd9-8286-fa20a3255776', metadata={'ayah_ar': 'يَٰٓأَيُّهَا ٱلَّذِينَ ءَامَنُوا۟ كُتِبَ عَلَيْكُمُ ٱلْقِصَاصُ فِى ٱلْقَتْلَى ۖ ٱلْحُرُّ بِٱلْحُرِّ وَٱلْعَبْدُ بِٱلْعَبْدِ وَٱلْأُنثَىٰ بِٱلْأُنثَىٰ ۚ فَمَنْ عُفِىَ لَهُۥ مِنْ أَخِيهِ شَىْءٌۭ فَٱتِّبَاعٌۢ بِٱلْمَعْرُوفِ وَأَدَآءٌ إِلَيْهِ بِإِحْسَٰنٍۢ ۗ ذَٰلِكَ تَخْفِيفٌۭ مِّن رَّبِّكُمْ وَرَحْمَةٌۭ ۗ فَمَنِ ٱعْتَدَىٰ بَعْدَ ذَٰلِكَ فَلَهُۥ عَذَابٌ أَلِيمٌۭ', 'ayah_en': 'O believers! ˹The law of˺ retaliation is set for you in cases of murder—a free man for a free man, a slave for a slave, and a female for a female. and payment should be made courteously. This is a concession and a mercy from your Lord. But whoever transgresses after that will suffer a painful punishment.', 'ayah_no_quran': 185.0, 'ayah_no_surah': 178.0, 'hizb_quarter': 11.0, 'juz_no': 2.0, 'list_of_words': '[يَٰٓأَيُّهَا,ٱلَّذِينَ,ءَامَنُوا۟,كُتِبَ,عَلَيْكُمُ,ٱلْقِصَاصُ,فِى,ٱلْقَتْلَى,ۖ,ٱلْحُرُّ,بِٱلْحُرِّ,وَٱلْعَبْدُ,بِٱلْعَبْدِ,وَٱلْأُنثَىٰ,بِٱلْأُنثَىٰ,ۚ,فَمَنْ,عُفِىَ,لَهُۥ,مِنْ,أَخِيهِ,شَىْءٌۭ,فَٱتِّبَاعٌۢ,بِٱلْمَعْرُوفِ,وَأَدَآءٌ,إِلَيْهِ,بِإِحْسَٰنٍۢ,ۗ,ذَٰلِكَ,تَخْفِيفٌۭ,مِّن,رَّبِّكُمْ,وَرَحْمَةٌۭ,ۗ,فَمَنِ,ٱعْتَدَىٰ,بَعْدَ,ذَٰلِكَ,فَلَهُۥ,عَذَابٌ,أَلِيمٌۭ]', 'manzil_no': 1.0, 'no_of_word_ayah': 41.0, 'place_of_revelation': 'Medinan', 'ruko_no': 23.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'البقرة', 'surah_name_en': 'The Cow', 'surah_name_roman': 'Al-Baqarah', 'surah_no': 2.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 286.0}, page_content='AyahEnglish: O believers! ˹The law of˺ retaliation is set for you in cases of murder—a free man for a free man, a slave for a slave, and a female for a female. and payment should be made courteously. This is a concession and a mercy from your Lord. But whoever transgresses after that will suffer a painful punishment.'),\n",
       " Document(id='a5fb26c6-d845-4338-92f2-2b9dfb1bfefd', metadata={'ayah_ar': 'وَٱلَّذِينَ لَا يَدْعُونَ مَعَ ٱللَّهِ إِلَٰهًا ءَاخَرَ وَلَا يَقْتُلُونَ ٱلنَّفْسَ ٱلَّتِى حَرَّمَ ٱللَّهُ إِلَّا بِٱلْحَقِّ وَلَا يَزْنُونَ ۚ وَمَن يَفْعَلْ ذَٰلِكَ يَلْقَ أَثَامًۭا', 'ayah_en': '˹They are˺ those who do not invoke any other god besides Allah, nor take a ˹human˺ life—made sacred by Allah—except with ˹legal˺ right, nor commit fornication. And whoever does ˹any of˺ this will face the penalty.', 'ayah_no_quran': 2923.0, 'ayah_no_surah': 68.0, 'hizb_quarter': 146.0, 'juz_no': 19.0, 'list_of_words': '[وَٱلَّذِينَ,لَا,يَدْعُونَ,مَعَ,ٱللَّهِ,إِلَٰهًا,ءَاخَرَ,وَلَا,يَقْتُلُونَ,ٱلنَّفْسَ,ٱلَّتِى,حَرَّمَ,ٱللَّهُ,إِلَّا,بِٱلْحَقِّ,وَلَا,يَزْنُونَ,ۚ,وَمَن,يَفْعَلْ,ذَٰلِكَ,يَلْقَ,أَثَامًۭا]', 'manzil_no': 4.0, 'no_of_word_ayah': 23.0, 'place_of_revelation': 'Meccan', 'ruko_no': 315.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'الفرقان', 'surah_name_en': 'The Criterion', 'surah_name_roman': 'Al-Furqan', 'surah_no': 25.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 77.0}, page_content='AyahArabic: وَٱلَّذِينَ لَا يَدْعُونَ مَعَ ٱللَّهِ إِلَٰهًا ءَاخَرَ وَلَا يَقْتُلُونَ ٱلنَّفْسَ ٱلَّتِى حَرَّمَ ٱللَّهُ إِلَّا بِٱلْحَقِّ وَلَا يَزْنُونَ ۚ وَمَن يَفْعَلْ ذَٰلِكَ يَلْقَ أَثَامًۭا\\nAyahEnglish: ˹They are˺ those who do not invoke any other god besides Allah, nor take a ˹human˺ life—made sacred by Allah—except with ˹legal˺ right, nor commit fornication. And whoever does ˹any of˺ this will face the penalty.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quran_vectorStore.similarity_search(\"morality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31febb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bDqMTqoaJ8dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "bDqMTqoaJ8dc",
    "outputId": "2c1debd4-192d-4279-cbdc-18a280e04ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.10)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.75 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai) (0.3.75)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-google-genai) (2.11.7)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.32.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.4.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain-google-genai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9dc32",
   "metadata": {},
   "source": [
    "7-BOOKS AHADITH ADDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a563533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'ahadith-index' already exists.\n"
     ]
    }
   ],
   "source": [
    "#bukhari index\n",
    "from pinecone import  ServerlessSpec\n",
    "\n",
    "# 1. Initialize the Pinecone client\n",
    "# This uses your API key and environment to connect.\n",
    "\n",
    "\n",
    "# 2. Define your index parameters\n",
    "#index_name = \"bukhari-index\"#created\n",
    "index_name=\"ahadith-index\" # Choose a unique, descriptive name\n",
    "dimension = 1024 # Match the output dimension of your embedding model\n",
    "metric = \"cosine\" # Cosine similarity is a good default for semantic search\n",
    "\n",
    "# 3. Check if the index already exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating a new Pinecone index: {index_name}...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    print(\"Index created successfully!\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# 4. Now you have an active index that you can connect to.\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0902ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*==CREATED EMBEDDINGS FOR 7 HADITH BOOKS==*\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "embeddingModel= HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "vectorStore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"ahadith-index\",\n",
    "    embedding=embeddingModel,\n",
    "   \n",
    ")\n",
    "\n",
    "# vectorStore_bukhari.add_documents(documents=bukhariChunks) #1\n",
    "# vectorStore_muslim.add_documents(documents=muslimChunks)#2\n",
    "# vectorStore_nasai.add_documents(documents=nasaiChunks)#3\n",
    "# vectorStore_majah.add_documents(documents=ibnmajaChunks)#4\n",
    "# vectorStore_malik.add_documents(documents=muwattaChunks)#5\n",
    "# vectorStore_ahmed.add_documents(documents=ahmedChunks)#6\n",
    "# vectorStore_tirmidhi.add_documents(documents=tirmidhiChunks)#7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9194512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"bukhari\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"tirmidhi\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"sunan_nasai\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"ibnmajah\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"malik\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"ahmed\"))\n",
    "# print(vectorStore.similarity_search(\"what is islamic way of governance\",k=1,namespace=\"muslim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "HrLu5PWwIxTA",
   "metadata": {
    "collapsed": true,
    "id": "HrLu5PWwIxTA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# from google.colab import userdata\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# # Retrieve your API key from Colab secrets\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# # Initialize the Gemini model\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "\n",
    "# # Now you can use the 'llm' object to interact with the model\n",
    "# response = llm.invoke(\"What is a Large Language Model?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "teDmfHfTN7En",
   "metadata": {
    "id": "teDmfHfTN7En"
   },
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# # Assuming you have successfully run the previous cells to initialize these\n",
    "# # llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "# # vectorStore = PineconeVectorStore.from_existing_index(...)\n",
    "\n",
    "# # 1. Create a retriever from your vector store\n",
    "# retriever = vectorStore.as_retriever()\n",
    "\n",
    "# # 2. Create the RetrievalQA chain\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\", # This packs all the retrieved documents into one prompt\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# # 3. Use the chain to get a final, synthesized answer\n",
    "# query = \"What is the hadith about the best charity?\"\n",
    "# result = qa_chain.invoke(query)\n",
    "\n",
    "# # The result is a dictionary; the answer is in the 'result' key\n",
    "# print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "LR9yAIIjP6I_",
   "metadata": {
    "collapsed": true,
    "id": "LR9yAIIjP6I_"
   },
   "outputs": [],
   "source": [
    "# from ast import parse\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# parser=StrOutputParser()\n",
    "# question=\"What is the hadith about the best charity?\"\n",
    "# contextRetriever=vectorStore.similarity_search(question,k=5)\n",
    "# prompt=ChatPromptTemplate(\n",
    "#     messages=[\n",
    "#         SystemMessagePromptTemplate.from_template(\"You are a helpful assistant who answers questions about the hadith from the given context{context} ;try to keep answer brief unless asked by the user , provide complete hadith if user specifically ask about haditha nd later exapplain that hadith as per users question.Also whenever you quote a hadith show hadith number source and chapter from the metadata,and as per user request do show user arabic text of the hadith\"),\n",
    "\n",
    "#         HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "#     ]\n",
    "# )\n",
    "# chain=prompt|llm|parser\n",
    "# chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4bLcPYwWhDl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4bLcPYwWhDl",
    "outputId": "756117a4-ea0a-4396-84b6-71f153e194d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph-checkpoint-sqlite in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.11)\n",
      "Requirement already satisfied: aiosqlite>=0.20 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint-sqlite) (0.21.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.21 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint-sqlite) (2.1.1)\n",
      "Requirement already satisfied: sqlite-vec>=0.1.6 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint-sqlite) (0.1.6)\n",
      "Requirement already satisfied: langchain-core>=0.2.38 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.3.75)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.14.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.4.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint<3.0.0,>=2.0.21->langgraph-checkpoint-sqlite) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core langgraph>0.2.27\n",
    "\n",
    "!pip install langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6f42262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6feec282-09db-4615-83c9-dd55427a3ddd', metadata={'ayah_ar': '۞ إِنَّ ٱللَّهَ يَأْمُرُ بِٱلْعَدْلِ وَٱلْإِحْسَٰنِ وَإِيتَآئِ ذِى ٱلْقُرْبَىٰ وَيَنْهَىٰ عَنِ ٱلْفَحْشَآءِ وَٱلْمُنكَرِ وَٱلْبَغْىِ ۚ يَعِظُكُمْ لَعَلَّكُمْ تَذَكَّرُونَ', 'ayah_en': 'Indeed, Allah commands justice, grace, as well as courtesy to close relatives. He forbids indecency, wickedness, and aggression. He instructs you so perhaps you will be mindful.', 'ayah_no_quran': 1991.0, 'ayah_no_surah': 90.0, 'hizb_quarter': 111.0, 'juz_no': 14.0, 'list_of_words': '[۞,إِنَّ,ٱللَّهَ,يَأْمُرُ,بِٱلْعَدْلِ,وَٱلْإِحْسَٰنِ,وَإِيتَآئِ,ذِى,ٱلْقُرْبَىٰ,وَيَنْهَىٰ,عَنِ,ٱلْفَحْشَآءِ,وَٱلْمُنكَرِ,وَٱلْبَغْىِ,ۚ,يَعِظُكُمْ,لَعَلَّكُمْ,تَذَكَّرُونَ]', 'manzil_no': 3.0, 'no_of_word_ayah': 18.0, 'place_of_revelation': 'Meccan', 'ruko_no': 236.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'النحل', 'surah_name_en': 'The Bee', 'surah_name_roman': 'An-Nahl', 'surah_no': 16.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 128.0}, page_content='AyahArabic: ۞ إِنَّ ٱللَّهَ يَأْمُرُ بِٱلْعَدْلِ وَٱلْإِحْسَٰنِ وَإِيتَآئِ ذِى ٱلْقُرْبَىٰ وَيَنْهَىٰ عَنِ ٱلْفَحْشَآءِ وَٱلْمُنكَرِ وَٱلْبَغْىِ ۚ يَعِظُكُمْ لَعَلَّكُمْ تَذَكَّرُونَ\\nAyahEnglish: Indeed, Allah commands justice, grace, as well as courtesy to close relatives. He forbids indecency, wickedness, and aggression. He instructs you so perhaps you will be mindful.'),\n",
       " Document(id='faba2f47-f6c5-4aa4-8319-a09fd36ccc57', metadata={'ayah_ar': 'يَٰٓأَيُّهَا ٱلَّذِينَ ءَامَنُوا۟ كُونُوا۟ قَوَّٰمِينَ لِلَّهِ شُهَدَآءَ بِٱلْقِسْطِ ۖ وَلَا يَجْرِمَنَّكُمْ شَنَـَٔانُ قَوْمٍ عَلَىٰٓ أَلَّا تَعْدِلُوا۟ ۚ ٱعْدِلُوا۟ هُوَ أَقْرَبُ لِلتَّقْوَىٰ ۖ وَٱتَّقُوا۟ ٱللَّهَ ۚ إِنَّ ٱللَّهَ خَبِيرٌۢ بِمَا تَعْمَلُونَ', 'ayah_en': 'O believers! Stand firm for Allah and bear true testimony. Do not let the hatred of a people lead you to injustice. Be just! That is closer to righteousness. And be mindful of Allah. Surely Allah is All-Aware of what you do.', 'ayah_no_quran': 677.0, 'ayah_no_surah': 8.0, 'hizb_quarter': 43.0, 'juz_no': 6.0, 'list_of_words': '[يَٰٓأَيُّهَا,ٱلَّذِينَ,ءَامَنُوا۟,كُونُوا۟,قَوَّٰمِينَ,لِلَّهِ,شُهَدَآءَ,بِٱلْقِسْطِ,ۖ,وَلَا,يَجْرِمَنَّكُمْ,شَنَـَٔانُ,قَوْمٍ,عَلَىٰٓ,أَلَّا,تَعْدِلُوا۟,ۚ,ٱعْدِلُوا۟,هُوَ,أَقْرَبُ,لِلتَّقْوَىٰ,ۖ,وَٱتَّقُوا۟,ٱللَّهَ,ۚ,إِنَّ,ٱللَّهَ,خَبِيرٌۢ,بِمَا,تَعْمَلُونَ]', 'manzil_no': 2.0, 'no_of_word_ayah': 30.0, 'place_of_revelation': 'Medinan', 'ruko_no': 87.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'المائدة', 'surah_name_en': 'The Table Spread', 'surah_name_roman': \"Al-Ma'idah\", 'surah_no': 5.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 120.0}, page_content='AyahEnglish: O believers! Stand firm for Allah and bear true testimony. Do not let the hatred of a people lead you to injustice. Be just! That is closer to righteousness. And be mindful of Allah. Surely Allah is All-Aware of what you do.'),\n",
       " Document(id='7a05f744-df71-4bd9-8286-fa20a3255776', metadata={'ayah_ar': 'يَٰٓأَيُّهَا ٱلَّذِينَ ءَامَنُوا۟ كُتِبَ عَلَيْكُمُ ٱلْقِصَاصُ فِى ٱلْقَتْلَى ۖ ٱلْحُرُّ بِٱلْحُرِّ وَٱلْعَبْدُ بِٱلْعَبْدِ وَٱلْأُنثَىٰ بِٱلْأُنثَىٰ ۚ فَمَنْ عُفِىَ لَهُۥ مِنْ أَخِيهِ شَىْءٌۭ فَٱتِّبَاعٌۢ بِٱلْمَعْرُوفِ وَأَدَآءٌ إِلَيْهِ بِإِحْسَٰنٍۢ ۗ ذَٰلِكَ تَخْفِيفٌۭ مِّن رَّبِّكُمْ وَرَحْمَةٌۭ ۗ فَمَنِ ٱعْتَدَىٰ بَعْدَ ذَٰلِكَ فَلَهُۥ عَذَابٌ أَلِيمٌۭ', 'ayah_en': 'O believers! ˹The law of˺ retaliation is set for you in cases of murder—a free man for a free man, a slave for a slave, and a female for a female. and payment should be made courteously. This is a concession and a mercy from your Lord. But whoever transgresses after that will suffer a painful punishment.', 'ayah_no_quran': 185.0, 'ayah_no_surah': 178.0, 'hizb_quarter': 11.0, 'juz_no': 2.0, 'list_of_words': '[يَٰٓأَيُّهَا,ٱلَّذِينَ,ءَامَنُوا۟,كُتِبَ,عَلَيْكُمُ,ٱلْقِصَاصُ,فِى,ٱلْقَتْلَى,ۖ,ٱلْحُرُّ,بِٱلْحُرِّ,وَٱلْعَبْدُ,بِٱلْعَبْدِ,وَٱلْأُنثَىٰ,بِٱلْأُنثَىٰ,ۚ,فَمَنْ,عُفِىَ,لَهُۥ,مِنْ,أَخِيهِ,شَىْءٌۭ,فَٱتِّبَاعٌۢ,بِٱلْمَعْرُوفِ,وَأَدَآءٌ,إِلَيْهِ,بِإِحْسَٰنٍۢ,ۗ,ذَٰلِكَ,تَخْفِيفٌۭ,مِّن,رَّبِّكُمْ,وَرَحْمَةٌۭ,ۗ,فَمَنِ,ٱعْتَدَىٰ,بَعْدَ,ذَٰلِكَ,فَلَهُۥ,عَذَابٌ,أَلِيمٌۭ]', 'manzil_no': 1.0, 'no_of_word_ayah': 41.0, 'place_of_revelation': 'Medinan', 'ruko_no': 23.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'البقرة', 'surah_name_en': 'The Cow', 'surah_name_roman': 'Al-Baqarah', 'surah_no': 2.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 286.0}, page_content='AyahEnglish: O believers! ˹The law of˺ retaliation is set for you in cases of murder—a free man for a free man, a slave for a slave, and a female for a female. and payment should be made courteously. This is a concession and a mercy from your Lord. But whoever transgresses after that will suffer a painful punishment.'),\n",
       " Document(id='a5fb26c6-d845-4338-92f2-2b9dfb1bfefd', metadata={'ayah_ar': 'وَٱلَّذِينَ لَا يَدْعُونَ مَعَ ٱللَّهِ إِلَٰهًا ءَاخَرَ وَلَا يَقْتُلُونَ ٱلنَّفْسَ ٱلَّتِى حَرَّمَ ٱللَّهُ إِلَّا بِٱلْحَقِّ وَلَا يَزْنُونَ ۚ وَمَن يَفْعَلْ ذَٰلِكَ يَلْقَ أَثَامًۭا', 'ayah_en': '˹They are˺ those who do not invoke any other god besides Allah, nor take a ˹human˺ life—made sacred by Allah—except with ˹legal˺ right, nor commit fornication. And whoever does ˹any of˺ this will face the penalty.', 'ayah_no_quran': 2923.0, 'ayah_no_surah': 68.0, 'hizb_quarter': 146.0, 'juz_no': 19.0, 'list_of_words': '[وَٱلَّذِينَ,لَا,يَدْعُونَ,مَعَ,ٱللَّهِ,إِلَٰهًا,ءَاخَرَ,وَلَا,يَقْتُلُونَ,ٱلنَّفْسَ,ٱلَّتِى,حَرَّمَ,ٱللَّهُ,إِلَّا,بِٱلْحَقِّ,وَلَا,يَزْنُونَ,ۚ,وَمَن,يَفْعَلْ,ذَٰلِكَ,يَلْقَ,أَثَامًۭا]', 'manzil_no': 4.0, 'no_of_word_ayah': 23.0, 'place_of_revelation': 'Meccan', 'ruko_no': 315.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'الفرقان', 'surah_name_en': 'The Criterion', 'surah_name_roman': 'Al-Furqan', 'surah_no': 25.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 77.0}, page_content='AyahArabic: وَٱلَّذِينَ لَا يَدْعُونَ مَعَ ٱللَّهِ إِلَٰهًا ءَاخَرَ وَلَا يَقْتُلُونَ ٱلنَّفْسَ ٱلَّتِى حَرَّمَ ٱللَّهُ إِلَّا بِٱلْحَقِّ وَلَا يَزْنُونَ ۚ وَمَن يَفْعَلْ ذَٰلِكَ يَلْقَ أَثَامًۭا\\nAyahEnglish: ˹They are˺ those who do not invoke any other god besides Allah, nor take a ˹human˺ life—made sacred by Allah—except with ˹legal˺ right, nor commit fornication. And whoever does ˹any of˺ this will face the penalty.'),\n",
       " Document(id='60c9a977-3534-44d2-b580-d658a616edbc', metadata={'ayah_ar': 'وَإِن طَآئِفَتَانِ مِنَ ٱلْمُؤْمِنِينَ ٱقْتَتَلُوا۟ فَأَصْلِحُوا۟ بَيْنَهُمَا ۖ فَإِنۢ بَغَتْ إِحْدَىٰهُمَا عَلَى ٱلْأُخْرَىٰ فَقَٰتِلُوا۟ ٱلَّتِى تَبْغِى حَتَّىٰ تَفِىٓءَ إِلَىٰٓ أَمْرِ ٱللَّهِ ۚ فَإِن فَآءَتْ فَأَصْلِحُوا۟ بَيْنَهُمَا بِٱلْعَدْلِ وَأَقْسِطُوٓا۟ ۖ إِنَّ ٱللَّهَ يُحِبُّ ٱلْمُقْسِطِينَ', 'ayah_en': 'And if two groups of believers fight each other, then make peace between them. But if one of them transgresses against the other, then fight against the transgressing group until they ˹are willing to˺ submit to the rule of Allah. If they do so, then make peace between both ˹groups˺ in all fairness and act justly. Surely Allah loves those who uphold justice.', 'ayah_no_quran': 4621.0, 'ayah_no_surah': 9.0, 'hizb_quarter': 206.0, 'juz_no': 26.0, 'list_of_words': '[وَإِن,طَآئِفَتَانِ,مِنَ,ٱلْمُؤْمِنِينَ,ٱقْتَتَلُوا۟,فَأَصْلِحُوا۟,بَيْنَهُمَا,ۖ,فَإِنۢ,بَغَتْ,إِحْدَىٰهُمَا,عَلَى,ٱلْأُخْرَىٰ,فَقَٰتِلُوا۟,ٱلَّتِى,تَبْغِى,حَتَّىٰ,تَفِىٓءَ,إِلَىٰٓ,أَمْرِ,ٱللَّهِ,ۚ,فَإِن,فَآءَتْ,فَأَصْلِحُوا۟,بَيْنَهُمَا,بِٱلْعَدْلِ,وَأَقْسِطُوٓا۟,ۖ,إِنَّ,ٱللَّهَ,يُحِبُّ,ٱلْمُقْسِطِينَ]', 'manzil_no': 6.0, 'no_of_word_ayah': 33.0, 'place_of_revelation': 'Medinan', 'ruko_no': 450.0, 'sajdah_ayah': 0.0, 'sajdah_no': 0.0, 'surah_name_ar': 'الحجرات', 'surah_name_en': 'The Rooms', 'surah_name_roman': 'Al-Hujurat', 'surah_no': 49.0, 'total_ayah_quran': 6236.0, 'total_ayah_surah': 18.0}, page_content='AyahEnglish: And if two groups of believers fight each other, then make peace between them. But if one of them transgresses against the other, then fight against the transgressing group until they ˹are willing to˺ submit to the rule of Allah. If they do so, then make peace between both ˹groups˺ in all fairness and act justly. Surely Allah loves those who uphold justice.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quran_vectorStore.similarity_search(\"morality\",k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "XF-W3I7sdNta",
   "metadata": {
    "id": "XF-W3I7sdNta"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, Sequence, Optional\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- LLM ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "#--- Prompt ---\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\"You are **ISLAM AI ASSISTANT**.  \n",
    "Your rules:  \n",
    "- Only answer from the provided **context: {context}**.  \n",
    "- Apply the **filter: {Filter}** → only use sources allowed by the filter.  \n",
    "- If you cannot anser from the context or messages history  → respond with an apology.  \n",
    "- Provide metadata:  \n",
    "  - If **Quran** → show Arabic + English + surah/ayah reference.  \n",
    "  - If **Hadith** → show English text + Arabic + book + chapter in arabic and english + grade.  \n",
    "- Answers must be **brief, direct, and to the question**.  \n",
    "- Never use your own knowledge or external data.  \n",
    "- Use past **message history** for consistency.  \n",
    "- Never provide Quran or Hadith outside the context.\n",
    "- In case where answer in hadith or quran or both is not found , mark all the field of it as none and in summary apologize i couldnt find the answer.                                                 \n",
    "- Summary should be a direct answer to the question based on the ayah and hadith in your response, always give **verified scholarly analysis**.  \n",
    "- BOOK NAME AS BOOK GIVE COMPLETE BOOK NAME LIKE SAHIH AL MUSLIM\n",
    "- chapEng is english chapter name\n",
    "- chapAr is arabic chapter name\n",
    "- you must return book,chapter i.e chapEng ,chapter_ar i.e chapArabic                                                  \n",
    "Always strictly format output in the following JSON format:  \n",
    "\n",
    "{{\n",
    "  \"summary\": \"direct answering the user's question\",\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"type\": \"Quran\",\n",
    "      \"surah\": \"Al-Ikhlas\",\n",
    "      \"ayahNumber\": \"1-4\",\n",
    "      \"ayahArabic\": \"قُلْ هُوَ ٱللَّهُ أَحَدٌ\",\n",
    "      \"ayahEnglish\": \"Say: He is Allah, One.\",\n",
    "      \"reference\": \"Quran 112:1\"\n",
    "    }},\n",
    "    {{\n",
    "      \"type\": \"Hadith\",\n",
    "      \"book\": \"Sahih al-Bukhari\",\n",
    "      \"chapter\": \"the chapter of governance\",\n",
    "      \"chapter_ar\": \" كتاب أحاديث الأنبياء\",\n",
    "      \"grade\": \"Sahih\",\n",
    "      \"textArabic\": \"إِنَّمَا الأَعْمَالُ بِالنِّيَّاتِ\",\n",
    "      \"textEnglish\": \"Actions are judged by intentions.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- State schema ---\n",
    "class RAGState(TypedDict):\n",
    "    messages: Annotated[list, lambda x, y: x + y]          # auto-accumulates\n",
    "    documents: Sequence[Document]                          # keep as list[Document]\n",
    "    filters: Annotated[Optional[list[str]], lambda x, y: y or x]  # selected sources\n",
    "\n",
    "\n",
    "# --- Retriever ---\n",
    "def retrieveDocuments(state: RAGState):\n",
    "    question = state[\"messages\"][-1].content\n",
    "    retrieved_docs: list[Document] = []\n",
    "\n",
    "    filter_map = {\n",
    "        \"bukhari\": (\"vectorStore\", \"bukhari\", 4),\n",
    "        \"muslim\": (\"vectorStore\", \"muslim\", 4),\n",
    "        \"tirmidhi\": (\"vectorStore\", \"tirmidhi\", 4),\n",
    "        \"sunan_nasai\": (\"vectorStore\", \"sunan_nasai\", 4),\n",
    "        \"malik\": (\"vectorStore\", \"malik\", 4),\n",
    "        \"ahmed\": (\"vectorStore\", \"ahmed\", 4),\n",
    "        \"ibnmajah\": (\"vectorStore\", \"ibnmajah\", 4),\n",
    "        \"Quran\": (\"Quran_vectorStore\", None, 5),\n",
    "    }\n",
    "\n",
    "    filters = state.get(\"filters\") or list(filter_map.keys())\n",
    "    if \"All\" in filters:\n",
    "        filters = list(filter_map.keys())\n",
    "\n",
    "    for f in filters:\n",
    "        store, namespace, k = filter_map.get(f, (None, None, None))\n",
    "        if store == \"vectorStore\":\n",
    "            retrieved_docs += vectorStore.similarity_search(question, k=k, namespace=namespace)\n",
    "        elif store == \"Quran_vectorStore\":\n",
    "            retrieved_docs += Quran_vectorStore.similarity_search(question, k=k)\n",
    "\n",
    "    # The fix is here:\n",
    "    if state.get(\"documents\") is None:\n",
    "        state[\"documents\"] = []\n",
    "    \n",
    "    state[\"documents\"].append(retrieved_docs)\n",
    "    \n",
    "    if len(state[\"documents\"]) > 2:\n",
    "        state[\"documents\"] = state[\"documents\"][-2:]\n",
    "\n",
    "    return {\"documents\": state[\"documents\"]}\n",
    "\n",
    "def flatten(docs):\n",
    "\n",
    "    for d in docs:\n",
    "        if d is None:\n",
    "            continue  # skip None values\n",
    "        if isinstance(d, list):\n",
    "            yield from flatten(d)  # recurse into nested lists\n",
    "        else:\n",
    "            yield d\n",
    "\n",
    "# --- Answer generator ---\n",
    "# --- Answer generator ---\n",
    "def generate_answer(state: RAGState):\n",
    "    # Keep last 20 messages\n",
    "    if len(state[\"messages\"]) > 20:\n",
    "        state[\"messages\"] = state[\"messages\"][-20:]\n",
    "\n",
    "    # Flatten nested documents\n",
    "    flat_documents = list(flatten(state[\"documents\"]))\n",
    "\n",
    "    documents_str = \"\"\n",
    "\n",
    "    if flat_documents:\n",
    "        # First (oldest) document(s)\n",
    "        documents_str += \"===  History of Documents (Older Context) ===\\n\\n\"\n",
    "        documents_str += (\n",
    "            f\"Book: {flat_documents[0].metadata.get('book','Unknown')}\\n\"\n",
    "            f\"Chapter (Arabic): {flat_documents[0].metadata.get('chapArabic','Unknown')}\\n\"\n",
    "            f\"Chapter (English): {flat_documents[0].metadata.get('chapEng','Unknown')}\\n\"\n",
    "            f\"Narrator: {flat_documents[0].metadata.get('narrator','Unknown')}\\n\"\n",
    "            f\"{flat_documents[0].page_content}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    if len(flat_documents) > 1:\n",
    "        # Remaining (newer/current) documents\n",
    "        documents_str += \"===  Current Document(s) Retrieved for the Question Context ===\\n\\n\"\n",
    "        for doc in flat_documents[1:]:   # <- start from index 1 to avoid repeating the oldest doc\n",
    "            md = doc.metadata or {}\n",
    "\n",
    "            # ✅ Detect Quran docs by their fields instead of relying on \"type\"\n",
    "            if \"surah_no\" in md and \"ayah_no_surah\" in md:\n",
    "                surah_en = md.get(\"surah_name_en\", \"Unknown\")\n",
    "                surah_ar = md.get(\"surah_name_ar\", \"\")\n",
    "                surah_no = int(md.get(\"surah_no\", 0)) if md.get(\"surah_no\") else None\n",
    "                ayah_no = int(md.get(\"ayah_no_surah\", 0)) if md.get(\"ayah_no_surah\") else None\n",
    "                ayah_ar = md.get(\"ayah_ar\") or md.get(\"AyahArabic\") or \"Unknown\"\n",
    "                ayah_en = md.get(\"ayah_en\") or md.get(\"AyahEnglish\") or \"Unknown\"\n",
    "\n",
    "                ref = f\"Quran {surah_no}:{ayah_no}\" if surah_no and ayah_no else \"Quran\"\n",
    "\n",
    "                documents_str += (\n",
    "                    f\"Type: Quran\\n\"\n",
    "                    f\"Surah: {surah_en} ({surah_ar})\\n\"\n",
    "                    f\"Surah Number: {surah_no}\\n\"\n",
    "                    f\"Ayah Number: {ayah_no}\\n\"\n",
    "                    f\"Ayah Arabic: {ayah_ar}\\n\"\n",
    "                    f\"Ayah English: {ayah_en}\\n\"\n",
    "                    f\"Reference: {ref}\\n\\n\"\n",
    "                )\n",
    "\n",
    "            elif md.get(\"type\") == \"Hadith\":\n",
    "                documents_str += (\n",
    "                    f\"Type: Hadith\\n\"\n",
    "                    f\"Book: {md.get('book','Unknown')}\\n\"\n",
    "                    f\"Chapter (English): {md.get('chapter','Unknown')}\\n\"\n",
    "                    f\"Chapter (Arabic): {md.get('chapter_ar','Unknown')}\\n\"\n",
    "                    f\"Grade: {md.get('grade','Unknown')}\\n\"\n",
    "                    f\"Text Arabic: {md.get('textArabic','Unknown')}\\n\"\n",
    "                    f\"Text English: {md.get('textEnglish','Unknown')}\\n\\n\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # Fallback (old structure)\n",
    "                documents_str += (\n",
    "                    f\"Book: {md.get('book','Unknown')}\\n\"\n",
    "                    f\"Chapter (Arabic): {md.get('chapArabic','Unknown')}\\n\"\n",
    "                    f\"Chapter (English): {md.get('chapEng','Unknown')}\\n\"\n",
    "                    f\"Narrator: {md.get('narrator','Unknown')}\\n\"\n",
    "                    f\"{doc.page_content}\\n\\n\"\n",
    "                )\n",
    "\n",
    "    print(\"string doc created\")\n",
    "\n",
    "    full_prompt = prompt.partial(context=documents_str, Filter=state[\"filters\"])\n",
    "\n",
    "    new_message_content = parser.invoke(\n",
    "        llm.invoke(full_prompt.invoke({\"messages\": state[\"messages\"]}))\n",
    "    )\n",
    "\n",
    "    state[\"messages\"] = [AIMessage(content=new_message_content)]\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "# --- Graph setup ---\n",
    "workflow = StateGraph(state_schema=RAGState)\n",
    "workflow.add_node(\"retriever\", retrieveDocuments)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# --- Checkpointing ---\n",
    "conn = sqlite3.connect(\"chatbot_memory.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f44418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "string doc created\n",
      "{\n",
      "  \"summary\": \"Allah is the Lord of the heavens and the earth, the Creator of all things, the One, the Supreme. He is the originator of creation, resurrects it, and provides for all from the heavens and the earth.\",\n",
      "  \"references\": [\n",
      "    {\n",
      "      \"type\": \"Quran\",\n",
      "      \"surah\": \"The Thunder (الرعد)\",\n",
      "      \"ayahNumber\": \"16\",\n",
      "      \"ayahArabic\": \"قُلْ مَن رَّبُّ ٱلسَّمَٰوَٰتِ وَٱلْأَرْضِ قُلِ ٱللَّهُ ۚ قُلْ أَفَٱتَّخَذْتُم مِّن دُونِهِۦٓ أَوْلِيَآءَ لَا يَمْلِكُونَ لِأَنفُسِهِمْ نَفْعًۭا وَلَا ضَرًّۭا ۚ قُلْ هَلْ يَسْتَوِى ٱلْأَعْمَىٰ وَٱلْبَصِيرُ أَمْ هَلْ تَسْتَوِى ٱلظُّلُمَٰتُ وَٱلنُّورُ ۗ أَمْ جَعَلُوا۟ لِلَّهِ شُرَكَآءَ خَلَقُوا۟ كَخَلْقِهِۦ فَتَشَٰبَهَ ٱلْخَلْقُ عَلَيْهِمْ ۚ قُلِ ٱللَّهُ خَٰلِقُ كُلِّ شَىْءٍۢ وَهُوَ ٱلْوَٰحِدُ ٱلْقَهَّٰرُ\",\n",
      "      \"ayahEnglish\": \"Ask ˹them, O Prophet˺, “Who is the Lord of the heavens and the earth?” Say, “Allah!” Ask ˹them˺, “Why ˹then˺ have you taken besides Him lords who cannot even benefit or protect themselves?” Say, “Can the blind and the sighted be equal? Or can darkness and light be equal?” Or have they associated with Allah partners who ˹supposedly˺ produced a creation like His, leaving them confused between the two creations? Say, “Allah is the Creator of all things, and He is the One, the Supreme.”\",\n",
      "      \"reference\": \"Quran 13:16\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"Quran\",\n",
      "      \"surah\": \"The Ant (النمل)\",\n",
      "      \"ayahNumber\": \"64\",\n",
      "      \"ayahArabic\": \"أَمَّن يَبْدَؤُا۟ ٱلْخَلْقَ ثُمَّ يُعِيدُهُۥ وَمَن يَرْزُقُكُم مِّنَ ٱلسَّمَآءِ وَٱلْأَرْضِ ۗ أَءِلَٰهٌۭ مَّعَ ٱللَّهِ ۚ قُلْ هَاتُوا۟ بُرْهَٰنَكُمْ إِن كُنتُمْ صَٰدِقِينَ\",\n",
      "      \"ayahEnglish\": \"Or ˹ask them,˺ “Who originates the creation then resurrects it, and gives you provisions from the heavens and the earth? Is it another god besides Allah?” Say, ˹O Prophet,˺ “Show ˹me˺ your proof, if what you say is true.”\",\n",
      "      \"reference\": \"Quran 27:64\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Example conversation ---\n",
    "thread_id = \"0uniu20001702345\"\n",
    "config2 = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "print(\"abc\")\n",
    "response3 = app.invoke(\n",
    "    {   \n",
    "        \"messages\": [HumanMessage(content=\"who is allah\")],\n",
    "        \"filters\": [\"Quran\",\"bukhari\",\"muslim\",\"malik\"],  # or \"All\"\n",
    "    },\n",
    "    config=config2,\n",
    ")\n",
    "\n",
    "print(response3[\"messages\"][-1].content.strip().removeprefix(\"```json\").removesuffix(\"```\").strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cleaned = response3['messages'][-1].content.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "cleaned\n",
    "data = json.loads(cleaned)\n",
    "data# response2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zEoUykD_9IFj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEoUykD_9IFj",
    "outputId": "fa92e2db-483a-4e3a-8479-1aa69439700c"
   },
   "outputs": [],
   "source": [
    "!pip install \"uvicorn[standard]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CZtfcxX-sRGm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZtfcxX-sRGm",
    "outputId": "7fee79c8-ad39-422f-ef6e-3d8e6bd66518"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from typing import Optional\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "App = FastAPI()\n",
    "\n",
    "App.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # or [\"http://localhost:3000\"]\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Allow Jupyter event loop to work with Uvicorn\n",
    "\n",
    "@App.post(\"/chat\")\n",
    "def HadithBot(payload: dict):\n",
    "    Question = payload.get(\"Question\")\n",
    "    filter_set = payload.get(\"filter\")\n",
    "\n",
    "\n",
    "    userID=payload.get(\"userID\")\n",
    "    BotResponse = app.invoke({\"messages\": [HumanMessage(content=Question)], \"filters\": filter_set}, config={\"configurable\": {\"thread_id\": userID}}\n",
    ")\n",
    "    cleaned = BotResponse['messages'][-1].content.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "    data=json.loads(cleaned)\n",
    "    return data\n",
    "\n",
    "uvicorn.run(App,host=\"127.0.0.1\",port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952002e872e70a96",
   "metadata": {
    "id": "952002e872e70a96"
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "#\n",
    "# # Download from the 🤗 Hub\n",
    "# matryoshka_dim = 786\n",
    "# model = SentenceTransformer(\"omarelshehy/arabic-english-sts-matryoshka-v2.0\", truncate_dim=matryoshka_dim)\n",
    "# # Run inference\n",
    "# sentences = [\n",
    "#     \"She enjoyed reading books by the window as the rain poured outside.\",\n",
    "#     \"كانت تستمتع بقراءة الكتب بجانب النافذة بينما كانت الأمطار تتساقط في الخارج.\",\n",
    "#     \"Reading by the window was her favorite thing, especially during rainy days.\"\n",
    "# ]\n",
    "# embeddings = model.encode(sentences)\n",
    "#\n",
    "# print(embeddings.shape)\n",
    "# # [3, 1024]\n",
    "#\n",
    "# # Get the similarity scores for the embeddings\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities.shape)\n",
    "# # [3, 3\n",
    "# embeddings[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d5da7",
   "metadata": {},
   "source": [
    "hadith chunkning of json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5bd123",
   "metadata": {},
   "source": [
    "BUkhari document formation and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"bukhari.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "bukhariDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId']  # numeric chapterId\n",
    "    arabicBook = data['chapters'][chap-1]['arabic']  # check actual structure!\n",
    "    engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'Sahih al-Bukhari'\n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    bukhariDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(docs))\n",
    "print(bukhariDocs[3865])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "bukhariChunks=splitter.split_documents(bukhariDocs)\n",
    "print(len(bukhariChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "bukhariChunks=splitter.split_documents(bukhariDocs)\n",
    "print(len(bukhariChunks))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e2092",
   "metadata": {},
   "source": [
    "    muslim document formation and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51902fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"muslim.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "muslimDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId']  # numeric chapterId\n",
    "    arabicBook = data['chapters'][chap-1]['arabic']  # check actual structure!\n",
    "    engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'Sahih al-Muslim'\n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    muslimDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(docs))\n",
    "print(muslimDocs[7279])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "muslimChunks=splitter.split_documents(muslimDocs)\n",
    "print(len(muslimChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "muslimChunks=splitter.split_documents(muslimDocs)\n",
    "print(len(muslimChunks))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287ae4e",
   "metadata": {},
   "source": [
    "tirmidhi formation and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61382793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"tirmidhi.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "tirmidhiDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId'] \n",
    "     # numeric chapterId\n",
    "    arabicBook = data['chapters'][chap-1]['arabic']  # check actual structure!\n",
    "    engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'Jami` at-Tirmidhi',\n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    tirmidhiDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(tirmidhiDocs))\n",
    "# print(tirmidhiDocs[4052])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "tirmidhiChunks=splitter.split_documents(tirmidhiDocs)\n",
    "print(len(tirmidhiChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "tirmidhiChunks=splitter.split_documents(tirmidhiDocs)\n",
    "print(len(tirmidhiChunks))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34bc21",
   "metadata": {},
   "source": [
    "muwatta malik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88161f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"malik.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "muwattaDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId'] \n",
    "     # numeric chapterId\n",
    "    arabicBook = data['chapters'][chap-1]['arabic']  # check actual structure!\n",
    "    engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'muwatta malik'\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    muwattaDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(muwattaDocs))\n",
    "print(muwattaDocs[188])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "muwattaChunks=splitter.split_documents(muwattaDocs)\n",
    "print(len(muwattaChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fe7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(muwattaDocs[108].metadata)\n",
    "print(muwattaDocs[1008].metadata)\n",
    "print(muwattaDocs[1508].metadata)\n",
    "print(muwattaDocs[300].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02521954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "muwattaChunks=splitter.split_documents(muwattaDocs)\n",
    "print(len(muwattaChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e034940",
   "metadata": {},
   "source": [
    "ibn e maja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e50b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"ibnmajah.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ibnmajaDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId'] \n",
    "     # numeric chapterId\n",
    "    arabicBook = data['chapters'][chap-1]['arabic']  # check actual structure!\n",
    "    engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'Ibn-e-majah'\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    ibnmajaDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(ibnmajaDocs))\n",
    "print(ibnmajaDocs[188])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "ibnmajaChunks=splitter.split_documents(ibnmajaDocs)\n",
    "print(len(ibnmajaChunks))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ibnmajaDocs[108].metadata)\n",
    "print(ibnmajaDocs[1818].metadata) \n",
    "print(ibnmajaDocs[2388].metadata) \n",
    "print(ibnmajaDocs[1008].metadata)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33453d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "ibnmajaChunks=splitter.split_documents(ibnmajaDocs)\n",
    "print(len(ibnmajaChunks))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d4ad5",
   "metadata": {},
   "source": [
    "Musnad AHMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea45736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"ahmed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ibnmajaDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId']\n",
    "    if chap>6: \n",
    "     # numeric chapterId\n",
    "        arabicBook = data['chapters'][7]['arabic']  # chapter_id 31 is at index 7\n",
    "        engBook = data['chapters'][7]['english']\n",
    "    else:    \n",
    "        arabicBook = data['chapters'][chap-1]['arabic']  # \n",
    "        engBook = data['chapters'][chap-1]['english']\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'musnad ahmed'\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    ibnmajaDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(ibnmajaDocs))\n",
    "# print(ibnmajaDocs[188])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chap=data['hadiths'][1373]['chapterId']\n",
    "data['chapters'][51]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fae90",
   "metadata": {},
   "source": [
    "sunan nasai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load JSON\n",
    "with open(\"nasai.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "nasaiDocs = []\n",
    "\n",
    "for hadith in data[\"hadiths\"]:\n",
    "    chap = hadith['chapterId']\n",
    "    if chap==None:\n",
    "        chap=35 #at chapter 35b it is None and at 35th index it is none so we simply change it to 35 \n",
    "    if(chap>35):\n",
    "        arabicBook = data['chapters'][chap]['arabic']  # after 34 chapter id ==index\n",
    "        engBook = data['chapters'][chap]['english']\n",
    "    else:\n",
    "     # numeric chapterId\n",
    "        arabicBook = data['chapters'][chap-1]['arabic']  # till 34 chapter id index is one less than index\n",
    "        engBook = data['chapters'][chap-1]['english']\n",
    "\n",
    "    # Page content should be string, not dict\n",
    "    page_content = f\"Arabic: {hadith['arabic']}\\nEnglish: {hadith['english']['text']}\"\n",
    "\n",
    "    # Metadata dictionary\n",
    "    metadata = {\n",
    "        'narrator': hadith['english']['narrator'],\n",
    "        'chapEng': engBook,\n",
    "        'chapArabic': arabicBook,\n",
    "        'book': 'Sunan nasai'\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Create Document object\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    nasaiDocs.append(doc)\n",
    "\n",
    "print(\"Total documents created:\", len(nasaiDocs))\n",
    "print(nasaiDocs[5767])  # preview first\n",
    "\n",
    "# # If chapters exist, check first one\n",
    "# if \"hadiths\" in data:\n",
    "#     print(data[96])   # keys inside first chapter\n",
    "#     print(json.dumps(data[\"hadiths\"][7276], indent=2, ensure_ascii=False))\n",
    "#     print(len(data[\"hadiths\"]))  # pretty-print first chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nasaiDocs[108].metadata)\n",
    "print(nasaiDocs[1818].metadata) \n",
    "print(nasaiDocs[2388].metadata) \n",
    "print(nasaiDocs[1008].metadata)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5a80c",
   "metadata": {},
   "source": [
    "Embeddings creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580ca37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfebcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*==CREATED EMBEDDINGS FOR 7 HADITH BOOKS==*\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "embeddingModel= HuggingFaceEmbeddings(model_name=\"omarelshehy/arabic-english-sts-matryoshka-v2.0\")\n",
    "\n",
    "vectorStore_bukhari = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"bukhari\"\n",
    ")\n",
    "\n",
    "vectorStore_muslim = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"muslim\"\n",
    ")\n",
    "vectorStore_nasai = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"sunan_nasai\"\n",
    ")\n",
    "vectorStore_majah = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"ibnmajah\"\n",
    ")\n",
    "vectorStore_malik = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"malik\"\n",
    ")\n",
    "vectorStore_ahmed = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"ahmed\"\n",
    ")\n",
    "vectorStore_tirmidhi = PineconeVectorStore(\n",
    "    index=pinecone_index,\n",
    "    embedding=embeddingModel,\n",
    "    namespace=\"tirmidhi\"\n",
    " )\n",
    "# vectorStore_bukhari.add_documents(documents=bukhariChunks) #1\n",
    "# vectorStore_muslim.add_documents(documents=muslimChunks)#2\n",
    "# vectorStore_nasai.add_documents(documents=nasaiChunks)#3\n",
    "# vectorStore_majah.add_documents(documents=ibnmajaChunks)#4\n",
    "# vectorStore_malik.add_documents(documents=muwattaChunks)#5\n",
    "# vectorStore_ahmed.add_documents(documents=ahmedChunks)#6\n",
    "# vectorStore_tirmidhi.add_documents(documents=tirmidhiChunks)#7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb8453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fb838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
